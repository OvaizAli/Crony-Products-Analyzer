{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e678cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'synthetic_retail_sales_2000_test.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Generating a synthetic dataset based on the given schema\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Number of records to generate\n",
    "# n_records = 2000\n",
    "\n",
    "# # Product mapping to ensure logical category assignments\n",
    "# product_category_mapping = {\n",
    "#     'Smartphone': 'Electronics',\n",
    "#     'Headphones': 'Electronics',\n",
    "#     'Monitor': 'Electronics',\n",
    "#     'Dining Table': 'Furniture',\n",
    "#     'Board Game': 'Toys',\n",
    "#     'Laptop': 'Electronics',\n",
    "#     'Sofa': 'Furniture',\n",
    "#     'Television': 'Electronics',\n",
    "#     'Camera': 'Electronics',\n",
    "#     'Watch': 'Clothing',\n",
    "#     'Blender': 'Kitchen',\n",
    "#     'Microwave': 'Kitchen',\n",
    "#     'T-shirt': 'Clothing',\n",
    "#     'Sneakers': 'Clothing',\n",
    "#     'Action Figure': 'Toys',\n",
    "#     'Coffee Maker': 'Kitchen',\n",
    "#     'Wardrobe': 'Furniture',\n",
    "#     'Bookshelf': 'Furniture',\n",
    "#     'Toy Car': 'Toys'\n",
    "# }\n",
    "\n",
    "# # Generate the columns\n",
    "# data = {\n",
    "#     'Transaction ID': ['T' + str(i) for i in range(1000, 1000 + n_records)],\n",
    "#     'Date': pd.date_range(start='2023-01-01', periods=n_records, freq='D'),\n",
    "#     'Product Name': np.random.choice(list(product_category_mapping.keys()), n_records),\n",
    "#     'Quantity Sold': np.random.randint(1, 20, n_records),\n",
    "#     'Sales Price per Unit ($)': np.round(np.random.uniform(10, 1000, n_records), 2),\n",
    "#     'Discount (%)': np.random.choice([0, 5, 10, 15, 20], n_records),\n",
    "#     'Stock After Sale': np.random.randint(1, 500, n_records),\n",
    "#     'Returned': np.random.choice([0, 1], n_records),\n",
    "#     'Weather Condition': np.random.choice(['Sunny', 'Rainy', 'Cloudy', 'Snow', 'Stormy'], n_records),\n",
    "#     'Temperature (°C)': np.round(np.random.uniform(-5, 40, n_records), 2),\n",
    "#     'Humidity (%)': np.random.randint(10, 100, n_records),\n",
    "#     'Precipitation (mm)': np.round(np.random.uniform(0, 50, n_records), 2),\n",
    "#     'Wind Speed (km/h)': np.round(np.random.uniform(0, 100, n_records), 2),\n",
    "#     'Is Holiday': np.random.choice([0, 1], n_records),\n",
    "#     'Is Weekend': np.random.choice([0, 1], n_records),\n",
    "#     'Is Special Event': np.random.choice([0, 1], n_records),\n",
    "#     'Season': np.random.choice(['Winter', 'Spring', 'Summer', 'Fall'], n_records)\n",
    "# }\n",
    "\n",
    "# # Assign Category based on Product Name\n",
    "# data['Category'] = [product_category_mapping[product] for product in data['Product Name']]\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Adjust Total Sales ($) based on quantity and price\n",
    "# df['Total Sales ($)'] = df['Quantity Sold'] * df['Sales Price per Unit ($)']\n",
    "# df['Total Sales After Discount ($)'] = df['Total Sales ($)'] * (1 - df['Discount (%)'] / 100)\n",
    "# df['Net Sales Value ($)'] = df['Total Sales After Discount ($)'] - df['Returned'] * df['Sales Price per Unit ($)']\n",
    "\n",
    "# # Creating lag features (for Quantity Sold) within each product\n",
    "# df['Sales_Lag_1'] = df.groupby('Product Name')['Quantity Sold'].shift(1)\n",
    "# df['Sales_Lag_7'] = df.groupby('Product Name')['Quantity Sold'].shift(7)\n",
    "# df['Sales_Lag_14'] = df.groupby('Product Name')['Quantity Sold'].shift(14)\n",
    "\n",
    "# # Create a future 7-day sales column as the target (simulating future sales)\n",
    "# df['Future_7_Day_Sales'] = df.groupby('Product Name')['Quantity Sold'].shift(-7)\n",
    "\n",
    "# # Drop the rows with NaN values due to lagging and future shifts\n",
    "# df.dropna(inplace=True)\n",
    "\n",
    "# # Save the dataset to CSV\n",
    "# file_path = 'synthetic_retail_sales_2000_test.csv'\n",
    "# df.to_csv(file_path, index=False)\n",
    "\n",
    "# file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cfb86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hyperopt scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f82afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset\n",
    "# data = pd.read_csv('synthetic_retail_sales_2000_with_lags_rational.csv')\n",
    "\n",
    "# # Function to check if the calculations are correct\n",
    "# def check_rationality(row):\n",
    "#     # Calculate Total Sales ($)\n",
    "#     calculated_total_sales = row['Quantity Sold'] * row['Sales Price per Unit ($)']\n",
    "    \n",
    "#     # Calculate Total Sales After Discount ($)\n",
    "#     calculated_total_sales_after_discount = calculated_total_sales * (1 - (row['Discount (%)'] / 100))\n",
    "    \n",
    "#     # Calculate Net Sales Value ($)\n",
    "#     calculated_net_sales_value = calculated_total_sales_after_discount - (row['Returned'] * row['Sales Price per Unit ($)'])\n",
    "\n",
    "#     # Sales Lag Calculations\n",
    "#     sales_lag_1_check = row['Sales_Lag_1'] == row['Quantity Sold']\n",
    "#     sales_lag_7_check = 'Sales_Lag_7' in data.columns and pd.notna(row['Sales_Lag_7']) and row['Sales_Lag_7'] >= 0\n",
    "#     sales_lag_14_check = 'Sales_Lag_14' in data.columns and pd.notna(row['Sales_Lag_14']) and row['Sales_Lag_14'] >= 0\n",
    "\n",
    "#     # Check for Future 7 Day Sales Prediction\n",
    "#     future_sales_check = 'Future_7_Day_Sales' in data.columns and pd.notna(row['Future_7_Day_Sales']) and row['Future_7_Day_Sales'] >= 0\n",
    "\n",
    "#     # Compare with the dataset values\n",
    "#     total_sales_check = abs(calculated_total_sales - row['Total Sales ($)']) < 1e-5\n",
    "#     total_sales_after_discount_check = abs(calculated_total_sales_after_discount - row['Total Sales After Discount ($)']) < 1e-5\n",
    "#     net_sales_value_check = abs(calculated_net_sales_value - row['Net Sales Value ($)']) < 1e-5\n",
    "\n",
    "#     return {\n",
    "#         'Transaction ID': row['Transaction ID'],\n",
    "#         'Total Sales Correct': total_sales_check,\n",
    "#         'Total Sales After Discount Correct': total_sales_after_discount_check,\n",
    "#         'Net Sales Value Correct': net_sales_value_check,\n",
    "#         'Sales Lag 1 Correct': sales_lag_1_check,\n",
    "#         'Sales Lag 7 Correct': sales_lag_7_check,\n",
    "#         'Sales Lag 14 Correct': sales_lag_14_check,\n",
    "#         'Future 7 Day Sales Correct': future_sales_check\n",
    "#     }\n",
    "\n",
    "# # Apply the check for each row in the dataset\n",
    "# results = data.apply(check_rationality, axis=1)\n",
    "\n",
    "# # Convert results to a DataFrame and display records with incorrect calculations\n",
    "# results_df = pd.DataFrame(results.tolist())\n",
    "\n",
    "# # Show only rows where any of the checks failed\n",
    "# incorrect_records = results_df[~(results_df['Total Sales Correct'] &\n",
    "#                                  results_df['Total Sales After Discount Correct'] &\n",
    "#                                  results_df['Net Sales Value Correct'] &\n",
    "#                                  results_df['Sales Lag 1 Correct'] &\n",
    "#                                  results_df['Sales Lag 7 Correct'] &\n",
    "#                                  results_df['Sales Lag 14 Correct'] &\n",
    "#                                  results_df['Future 7 Day Sales Correct'])]\n",
    "\n",
    "# if len(incorrect_records) == 0:\n",
    "#     print(\"All records are rationally correct.\")\n",
    "# else:\n",
    "#     print(\"The following records have incorrect calculations:\")\n",
    "#     print(incorrect_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149adc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:09<00:00,  1.30s/trial, best loss: 29.95025774815466]\n",
      "Best hyperparameters: {'max_depth': 2, 'max_features': 3, 'min_samples_leaf': 19, 'min_samples_split': 10, 'n_estimators': 2}\n",
      "Final Mean Squared Error: 29.66152762924642\n",
      "Predictions for new data: [9.63416067]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import joblib\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('synthetic_retail_sales_2000_with_lags_rational.csv')\n",
    "\n",
    "# Data cleaning (handle missing values, etc.)\n",
    "data = data.dropna()\n",
    "\n",
    "# Extract date-related features if needed (e.g., day, month)\n",
    "data['Day'] = pd.to_datetime(data['Date']).dt.day\n",
    "data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "data['Year'] = pd.to_datetime(data['Date']).dt.year\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "data = pd.get_dummies(data, columns=['Weather Condition', 'Season', 'Category', 'Product Name'], drop_first=True)\n",
    "\n",
    "# Define features and target, retaining relevant sales metrics\n",
    "features = data.drop(columns=['Transaction ID', 'Future_7_Day_Sales', 'Date'])  # Consider keeping sales metrics\n",
    "target = data['Future_7_Day_Sales']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the search space for hyperparameter optimization\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 200]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30]),  # None is acceptable, but no 0\n",
    "    'min_samples_split': hp.randint('min_samples_split', 2, 20),\n",
    "    'min_samples_leaf': hp.randint('min_samples_leaf', 1, 20),\n",
    "    'max_features': hp.choice('max_features', [0.5, 0.75, 'sqrt', 'log2', None])  # Valid options\n",
    "}\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(params):\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return {'loss': mse, 'status': STATUS_OK}\n",
    "\n",
    "# Create a Trials object to keep track of progress\n",
    "trials = Trials()\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = RandomForestRegressor(**best)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "final_predictions = final_model.predict(X_test)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "print(f'Final Mean Squared Error: {final_mse}')\n",
    "\n",
    "# Save the model and feature names using joblib\n",
    "joblib.dump(final_model, 'final_model.joblib')\n",
    "joblib.dump(features.columns, 'feature_names.joblib')\n",
    "\n",
    "# Load the model and feature names for future predictions\n",
    "loaded_model = joblib.load('final_model.joblib')\n",
    "loaded_feature_names = joblib.load('feature_names.joblib')\n",
    "\n",
    "# Create new data ensuring to include the necessary categorical columns\n",
    "new_data = pd.DataFrame({\n",
    "    'Temperature (°C)': [25],\n",
    "    'Humidity (%)': [70],\n",
    "    'Precipitation (mm)': [0],\n",
    "    'Wind Speed (km/h)': [10],\n",
    "    'Is Holiday': [0],\n",
    "    'Is Weekend': [0],\n",
    "    'Is Special Event': [1],\n",
    "    'Day': [15],  # Example day\n",
    "    'Month': [7],  # Example month\n",
    "    'Year': [2023],  # Example year\n",
    "    'Weather Condition': ['Sunny'],\n",
    "    'Season': ['Summer'],\n",
    "    'Category': ['Electronics'],\n",
    "    'Product Name': ['Smartphone'],\n",
    "})\n",
    "\n",
    "# One-hot encoding for categorical features in new data\n",
    "new_data = pd.get_dummies(new_data, columns=['Weather Condition', 'Season', 'Category', 'Product Name'], drop_first=True)\n",
    "\n",
    "# Align the new data with the trained model's features\n",
    "new_data = new_data.reindex(columns=loaded_feature_names, fill_value=0)\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(new_data)\n",
    "print(\"Predictions for new data:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
